{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, accuracy_score\n",
    "np.set_printoptions(threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import Data\n",
    "mnist_train = genfromtxt(\"/Users/schand/Documents/mnist_train.csv\",delimiter=',')\n",
    "mnist_test = genfromtxt(\"/Users/schand/Documents/mnist_test.csv\",delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = mnist_train.shape[0]\n",
    "m_test = mnist_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train, y_train = mnist_train[:,1:785], mnist_train[:,0]\n",
    "X_test, y_test = mnist_test[:,1:785], mnist_test[:,0]\n",
    "X_train, X_test = X_train[:m].T, X_test[:m_test].T\n",
    "#y_train, y_test = y_train[:m].reshape(1,m), y_test[:m_test].reshape(1,m_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just building a zero-classifier for now.\n",
    "y_train_new = np.zeros(y_train.shape)\n",
    "y_train_new[np.where(y_train == 0.0)[0]] = 1\n",
    "y_test_new = np.zeros(y_test.shape)\n",
    "y_test_new[np.where(y_test == 0.0)[0]] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    s = 1 / (1 + np.exp(-z))\n",
    "    return s\n",
    "def ReLU(z):\n",
    "    r = np.maximum(0,z)\n",
    "    return r\n",
    "def softmax(z):\n",
    "    expo = np.exp(z)\n",
    "    expo_sum = np.sum(np.exp(z))\n",
    "    return expo/expo_sum\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(Y, Y_hat):\n",
    "\n",
    "    L = -(1./m) * ( np.sum( np.multiply(np.log(Y_hat),Y) ) + np.sum( np.multiply(np.log(1-Y_hat),(1-Y)) ) )\n",
    "\n",
    "    return L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 cost:  0.3576621899656291\n",
      "Epoch 10 cost:  0.216723618326031\n",
      "Epoch 20 cost:  0.1526620995591674\n",
      "Epoch 30 cost:  0.10202617926768559\n",
      "Epoch 40 cost:  0.08333919296000918\n",
      "Epoch 50 cost:  0.07377817839997725\n",
      "Epoch 60 cost:  0.06662565831736365\n",
      "Epoch 70 cost:  0.060980393080738805\n",
      "Epoch 80 cost:  0.0563533768454155\n",
      "Epoch 90 cost:  0.052437476676484754\n",
      "Final cost: 0.04936507433644391\n"
     ]
    }
   ],
   "source": [
    "# we are building neral network with three hidden layer(256, 64, 16) with input 784 neurons and 1 output neurons\n",
    "\n",
    "X = X_train * 0.99/255 + 0.01 # normalize X_train in between 0.01 to 1\n",
    "Y = y_train_new\n",
    "\n",
    "n_x = X.shape[0]\n",
    "n_1 = 784\n",
    "n_2 = 256\n",
    "n_3 = 64\n",
    "n_4 = 16\n",
    "n_5 = 1\n",
    "\n",
    "learning_rate = 3\n",
    "\n",
    "W1 = np.random.randn(n_1, n_x)\n",
    "b1 = np.zeros((n_1, 1))\n",
    "W2 = np.random.randn(n_2, n_1)\n",
    "b2 = np.zeros((n_2, 1))\n",
    "W3 = np.random.randn(n_3,n_2)\n",
    "b3 = np.zeros((n_3,1))\n",
    "W4 = np.random.randn(n_4,n_3)\n",
    "b4 = np.zeros((n_4,1))\n",
    "W5 = np.random.randn(n_5,n_4)\n",
    "b5 = np.zeros((n_5,1))\n",
    "\n",
    "for i in range(100):\n",
    "    Z1 = np.matmul(W1, X) + b1\n",
    "    A1 = sigmoid(Z1)\n",
    "    Z2 = np.matmul(W2, A1) + b2\n",
    "    A2 = sigmoid(Z2)\n",
    "    Z3 = np.matmul(W3, A2) + b3\n",
    "    A3 = sigmoid(Z3)\n",
    "    Z4 = np.matmul(W4, A3) + b4\n",
    "    A4 = sigmoid(Z4)\n",
    "    Z5 = np.matmul(W5, A4) + b5\n",
    "    A5 = sigmoid(Z5)\n",
    "\n",
    "    Y_hat = A5\n",
    "    cost = compute_loss(Y, Y_hat)\n",
    "    \n",
    "\n",
    "    dZ5 = Y_hat-Y\n",
    "    dW5 = (1./m) * np.matmul(dZ5, A4.T)\n",
    "    db5 = (1./m) * np.sum(dZ5, axis=1, keepdims=True)\n",
    "        \n",
    "    dA4 = np.matmul(W5.T, dZ5)\n",
    "    dZ4 = dA4 * sigmoid(Z4) * (1 - sigmoid(Z4))\n",
    "    dW4 = (1./m) * np.matmul(dZ4, A3.T)\n",
    "    db4 = (1./m) * np.sum(dZ4, axis=1, keepdims=True)\n",
    "\n",
    "    dA3 = np.matmul(W4.T, dZ4)\n",
    "    dZ3 = dA3 * sigmoid(Z3) * (1 - sigmoid(Z3))\n",
    "    dW3 = (1./m) * np.matmul(dZ3, A2.T)\n",
    "    db3 = (1./m) * np.sum(dZ3, axis=1, keepdims=True)\n",
    "\n",
    "    dA2 = np.matmul(W3.T, dZ3)\n",
    "    dZ2 = dA2 * sigmoid(Z2) * (1 - sigmoid(Z2))\n",
    "    dW2 = (1./m) * np.matmul(dZ2, A1.T)\n",
    "    db2 = (1./m) * np.sum(dZ2, axis=1, keepdims=True)\n",
    "    \n",
    "    dA1 = np.matmul(W2.T, dZ2)\n",
    "    dZ1 = dA1 * sigmoid(Z1) * (1 - sigmoid(Z1))\n",
    "    dW1 = (1./m) * np.matmul(dZ1, X.T)\n",
    "    db1 = (1./m) * np.sum(dZ1, axis=1, keepdims=True)\n",
    "    \n",
    "    W5 = W5 - learning_rate * dW5\n",
    "    b5 = b5 - learning_rate * db5\n",
    "    W4 = W4 - learning_rate * dW4\n",
    "    b4 = b4 - learning_rate * db4\n",
    "    W3 = W3 - learning_rate * dW3\n",
    "    b3 = b3 - learning_rate * db3\n",
    "    W2 = W2 - learning_rate * dW2\n",
    "    b2 = b2 - learning_rate * db2\n",
    "    W1 = W1 - learning_rate * dW1\n",
    "    b1 = b1 - learning_rate * db1\n",
    "\n",
    "    if i % 10 == 0:\n",
    "        print(\"Epoch\", i, \"cost: \", cost)\n",
    "print(\"Final cost:\", cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z1 = np.matmul(W1, X) + b1\n",
    "A1 = sigmoid(Z1)\n",
    "Z2 = np.matmul(W2, A1) + b2\n",
    "A2 = sigmoid(Z2)\n",
    "Z3 = np.matmul(W3, A2) + b3\n",
    "A3 = sigmoid(Z3)\n",
    "Z4 = np.matmul(W4, A3) + b4\n",
    "A4 = sigmoid(Z4)\n",
    "Z5 = np.matmul(W5, A4) + b5\n",
    "A5 = sigmoid(Z5)\n",
    "Y_hat = (A5>0.5)[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix is [[53762   562]\n",
      " [  315  5361]]\n",
      "Accuracy Score is 0.9853833333333334\n"
     ]
    }
   ],
   "source": [
    "print('Confusion matrix is', confusion_matrix(Y_hat,Y))\n",
    "print('Accuracy Score is', accuracy_score(Y_hat,Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The final cost is 0.04936507433644391 which is considerably low with accuarcy score 0.9853833333333334. The cost function can be reduced by inceasing the epochs at the same time we should think of overfitting problem. I did only zero-classifier due to limited computaional power."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
